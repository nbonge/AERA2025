@article{Bassili1996,
   abstract = {This research explores the potential utility of response latency as an index of question problems in survey research. The time respondents took to answer three types of bad questions was compared to the time they took to answer the repaired versions of the questions. Questions containing a superfluous negative and double-barreled questions took longer to answer than nearly identical questions without these problems. Repaired versions of questions soliciting frequency estimates, however, took longer to answer than their problematic counterparts. The results are discussed in the context of a model of question answering, and their implications for survey methodology are explored. Survey methodologists have expressed considerable interest recently in techniques for screening survey questions with the aim of repairing bad questions before presenting them to large numbers of respondents (see Presser and Blair 1994). A number of approaches for the early identification of question problems have been explored. Observational monitoring focuses on the interaction between the interviewer and respondent and relies on a behavior-coding scheme to identify problems (e.g., Fowler and Cannell 19%). The cognitive interview is a method for gathering detailed information from respondents about the processes involved in the formulation of responses and involves extensive probing, either during the interview or immediately after it (e.g., Jobe, Tourangeau, and Smith 1993). Analysis of the verbal output based on "think aloud" protocols obtained from respondents retrospectively or while they answer questions has also been implemented, sometimes with automatic coding of the protocols (e.g., Bolton 1993). In addition, methods for coding the questionnaire itself have also been developed (e.g., Lessler and Forsyth 1996).},
   author = {John N Bassili and B Stacey Scott},
   isbn = {60/3/390/1832313},
   journal = {Public Opinion Quarterly},
   pages = {390-399},
   title = {Response latency as a signal to question problems in survey research},
   volume = {60},
   url = {https://academic.oup.com/poq/article/60/3/390/1832313},
   year = {1996}
}
@article{Bates2015,
   author = {Douglas Bates and Martin Mächler and Ben Bolker and Steve Walker},
   doi = {10.18637/jss.v067.i01},
   issn = {1548-7660},
   issue = {1},
   journal = {Journal of Statistical Software},
   pages = {1-48},
   title = {Fitting linear mixed-effects models using lme4},
   volume = {67},
   year = {2015}
}
@article{Beckmann2000,
   author = {Jens F. Beckmann},
   issue = {3},
   journal = {Diagnostica},
   pages = {124-129},
   title = {Differentielle Latenzzeitefekte bei der Bearbeitung von Reasoning-Items},
   volume = {46},
   year = {2000}
}
@article{Bowling2023,
   abstract = {Several recent studies have examined the prevention, causes, and consequences of insufficient effort responding (IER) to surveys. Scientific progress in this area, however, rests on the availability of construct-valid IER measures. In the current paper we describe the potential merits of the page time index, which is computed by counting the number of questionnaire pages to which a participant has responded more quickly than two seconds per item (see Huang et al., 2012). We conducted three studies (total N = 1,056) to examine the page time index's construct validity. Across these studies, we found that page time converged highly with other IER indices, that it was sensitive to an experimental manipulation warning participants to respond carefully, and that it predicted the extent to which participants were unable to recognize item content. We also found that page time's validity was superior to that of total completion time and that the two-seconds-per-item rule yielded a construct-valid page time score for items of various word lengths. Given its apparent validity, we provide practical recommendations for the use of the page time index.},
   author = {Nathan A. Bowling and Jason L. Huang and Cheyna K. Brower and Caleb B. Bragg},
   doi = {10.1177/10944281211056520},
   issn = {15527425},
   issue = {2},
   journal = {Organizational Research Methods},
   keywords = {careless responding,insufficient effort responding,participant inattention,random responding,response effort},
   month = {4},
   pages = {323-352},
   publisher = {SAGE Publications Inc.},
   title = {The Quick and the Careless: The Construct Validity of Page Time as a Measure of Insufficient Effort Responding to Surveys},
   volume = {26},
   year = {2023}
}
@article{Chalmers2012,
   author = {R. Philip Chalmers},
   doi = {10.18637/jss.v048.i06},
   issn = {1548-7660},
   issue = {6},
   journal = {Journal of Statistical Software},
   pages = {1-29},
   title = {mirt: A Multidimensional Item Response Theory Package for the R Environment},
   volume = {48},
   year = {2012}
}
@book{Christie1970,
   author = {Richard Christie and Florence L. Geis},
   doi = {10.1016/C2013-0-10497-7},
   isbn = {9780121744502},
   publisher = {Elsevier},
   title = {Studies in Machiavellianism},
   year = {1970}
}
@article{DeBoeck2019,
   abstract = {Response times (RTs) are a natural kind of data to investigate cognitive processes underlying cognitive test performance. We give an overview of modeling approaches and of findings obtained with these approaches. Four types of models are discussed: response time models (RT as the sole dependent variable), joint models (RT together with other variables as dependent variable), local dependency models (with remaining dependencies between RT and accuracy), and response time as covariate models (RT as independent variable). The evidence from these approaches is often not very informative about the specific kind of processes (other than problem solving, information accumulation, and rapid guessing), but the findings do suggest dual processing: automated processing (e.g., knowledge retrieval) vs. controlled processing (e.g., sequential reasoning steps), and alternative explanations for the same results exist. While it seems well-possible to differentiate rapid guessing from normal problem solving (which can be based on automated or controlled processing), further decompositions of response times are rarely made, although possible based on some of model approaches.},
   author = {Paul De Boeck and Minjeong Jeon},
   doi = {10.3389/fpsyg.2019.00102},
   issn = {16641078},
   issue = {FEB},
   journal = {Frontiers in Psychology},
   keywords = {Automated and controlled processes,Cognitive processes,Cognitive tests,Local dependencies,Psychometric models,Response accuracy,Response time},
   month = {2},
   publisher = {Frontiers Media S.A.},
   title = {An overview of models for response times and processes in cognitive tests},
   volume = {10},
   year = {2019}
}
@article{DeSimone2018,
   abstract = {The purpose of this study is to empirically address questions pertaining to the effects of data screening practices in survey research. This study addresses questions about the impact of screening techniques on data and statistical analyses. It also serves an initial attempt to estimate descriptive statistics and graphically display the distributions of popular screening techniques. Data were obtained from an online sample who completed demographic items and measures of character strengths (N = 307). Screening indices demonstrate minimal overlap and differ in the number of participants flagged. Existing cutoff scores for most screening techniques seem appropriate, but cutoff values for consistency-based indices may be too liberal. Screens differ in the extent to which they impact survey results. The use of screening techniques can impact inter-item correlations, inter-scale correlations, reliability estimates, and statistical results. While data screening can improve the quality and trustworthiness of data, screening techniques are not interchangeable. Researchers and practitioners should be aware of the differences between data screening techniques and apply appropriate screens for their survey characteristics and study design. Low-impact direct and unobtrusive screens such as self-report indicators, bogus items, instructed items, longstring, individual response variability, and response time are relatively simple to administer and analyze. The fact that data screening can influence the statistical results of a study demonstrates that low-quality data can distort hypothesis testing in organizational research and practice. We recommend analyzing results both before and after screens have been applied.},
   author = {Justin A. DeSimone and P. D. Harms},
   doi = {10.1007/s10869-017-9514-9},
   issn = {08893268},
   issue = {5},
   journal = {Journal of Business and Psychology},
   keywords = {Data analysis,Data screening,Insufficient effort responding,Research design,Research methods,Survey research},
   month = {10},
   pages = {559-577},
   publisher = {Springer New York LLC},
   title = {Dirty Data: The Effects of Screening Respondents Who Provide Low-Quality Data in Survey Research},
   volume = {33},
   year = {2018}
}
@article{Ferrando2007,
   abstract = {This article describes a model for response times that is proposed as a supplement to the usual factor-analytic model for responses to graded or more continuous typical-response items. The use of the proposed model together with the factor model provides additional information about the respondent and can potentially increase the accuracy of the individual trait estimates. First, the rationale of the model is discussed in relation to previous developments in binary responses. Sec- ond, procedures for fitting the model and for assessing model-data fit at both overall and individual level (person-fit) are proposed. Third, the usefulness of the model and its potential applications in the typical-response domain are discussed. All the proposed developments are used in 2 empirical applications in the person- ality domain. The first application analyzes 2 scales from a Big Five questionnaire. The second example analyzes a sociability scale developed from Eysenck’s ques- tionnaires.},
   author = {Pere J. Ferrando and Urbano Lorenzo-Seva},
   doi = {10.1080/00273170701710247},
   issn = {0027-3171},
   issue = {4},
   journal = {Multivariate Behavioral Research},
   month = {12},
   pages = {675-706},
   publisher = {Informa UK Limited},
   title = {A Measurement Model for Likert Responses That Incorporates Response Time},
   volume = {42},
   year = {2007}
}
@article{Ferrando2006,
   abstract = {'Rovira i Virgili' University (Spain) This study assessed the hypothesis that the response time to an item increases as the positions of the item and the respondent on the continuum of the trait that is measured draw closer together. This hypothesis has previously been stated by several authors, but so far it does not seem to have been empirically assessed in a rigorous way. A computerized version of a 22-item two-scale personality questionnaire was administered to a sample of 286 respondents. The item responses were fitted using the two-parameter IRT model and a person-item distance measure was derived. Product-moment correlations between the log-response times and the person-item distances were obtained over respondents within each item. In both scales all the correlations were negative, as expected from the theory. However, most of the correlations (effect sizes) were small. The potential usefulness of the results for personality measurement is discussed.},
   author = {Pere J Ferrando},
   journal = {Psicológica},
   pages = {137-148},
   title = {Person-item distance and response time: An empirical study in personality measurement},
   volume = {27},
   year = {2006}
}
@misc{Ferrando2007,
   abstract = {This article describes a general item response theory model for personality items that allows the information provided by the item response times to be used to estimate the individual trait levels. The submodel describing the item response times is a modification of Thissen's log-linear model and is based on the distance-difficulty hypothesis in personality measurement. First, the procedures for fitting the model and assessing the goodness of fit are described. Second, the gain in the precision of estimating the individual trait levels when the information provided by the response times is used is assessed. Finally, all the developments in this article are illustrated by means of an empirical example. © 2007 Sage Publications.},
   author = {Pere J. Ferrando and Urbano Lorenzo-Seva},
   doi = {10.1177/0146621606295197},
   issn = {01466216},
   issue = {6},
   journal = {Applied Psychological Measurement},
   keywords = {Distance-difficulty hypothesis,Item response theory,Item response times,Personality measurement},
   month = {11},
   pages = {525-543},
   title = {An item response theory model for incorporating response time data in binary personality items},
   volume = {31},
   year = {2007}
}
@book{Fox2019,
   author = {John Fox and Sanford Weisberg},
   city = {Thousand Oaks, CA},
   edition = {Third},
   publisher = {Sage},
   title = {An R Companion to Applied Regression},
   year = {2019}
}
@inbook{Gilhooly2021,
   abstract = {Adult age is associated with relatively intact crystallized abilities but with declines in fluid abilities, in working memory and in learning. Reduced processing speed may explain these changes and could be due to a range of neurological and cardiovascular changes. Hearing impairment is correlated with cognitive impairment and risk of Alzheimer's disease (AD). Difficulties with visual pattern restructuring with age may impact on insight problems. Negative age effects are less with real-world problems, and some positive age effects have been found on “wisdom” tasks. Personality measures relevant to creativity were found to be remarkably stable over many years and into old age in longitudinal studies. However, there were small declining average trends, particularly in openness, and some variability over time, linked to life events such as retirement and illness. A U-shaped curve for happiness with age with a final declining phase in late life. Age led to more broken sleep, with longer time in rapid eye movement (REM) but less in slow wave sleep (SWS). AD and other dementias generally have a devastating effect on creativity. Early and mid-life engagement with cognitively demanding activities is associated with lower risks of later cognitive decline and Alzheimer's disease.},
   author = {Kenneth J. Gilhooly and Mary L.M. Gilhooly},
   doi = {10.1016/b978-0-12-816401-3.00008-1},
   booktitle = {Aging and Creativity},
   pages = {183-216},
   publisher = {Elsevier},
   title = {Aging effects on cognitive and noncognitive factors in creativity},
   year = {2021}
}
@article{Hirsch2021,
   abstract = {Although the neural systems that underlie spoken language are well-known, how they adapt to evolving social cues during natural conversations remains an unanswered question. In this work we investigate the neural correlates of face-to-face conversations between two individuals using functional near infrared spectroscopy (fNIRS) and acoustical analyses of concurrent audio recordings. Nineteen pairs of healthy adults engaged in live discussions on two controversial topics where their opinions were either in agreement or disagreement. Participants were matched according to their a priori opinions on these topics as assessed by questionnaire. Acoustic measures of the recorded speech including the fundamental frequency range, median fundamental frequency, syllable rate, and acoustic energy were elevated during disagreement relative to agreement. Consistent with both the a priori opinion ratings and the acoustic findings, neural activity associated with long-range functional networks, rather than the canonical language areas, was also differentiated by the two conditions. Specifically, the frontoparietal system including bilateral dorsolateral prefrontal cortex, left supramarginal gyrus, angular gyrus, and superior temporal gyrus showed increased activity while talking during disagreement. In contrast, talking during agreement was characterized by increased activity in a social and attention network including right supramarginal gyrus, bilateral frontal eye-fields, and left frontopolar regions. Further, these social and visual attention networks were more synchronous across brains during agreement than disagreement. Rather than localized modulation of the canonical language system, these findings are most consistent with a model of distributed and adaptive language-related processes including cross-brain neural coupling that serves dynamic verbal exchanges.},
   author = {Joy Hirsch and Mark Tiede and Xian Zhang and J. Adam Noah and Alexandre Salama-Manteau and Maurice Biriotti},
   doi = {10.3389/fnhum.2020.606397},
   issn = {16625161},
   journal = {Frontiers in Human Neuroscience},
   keywords = {acoustical analysis,adaptive models of language,agreement and disagreement,dynamic spoken language,hyperscanning,near-infrared spectroscopy,neural coupling,two-person neuroscience},
   month = {1},
   publisher = {Frontiers Media S.A.},
   title = {Interpersonal agreement and disagreement during face-to-face dialogue: An fNIRS investigation},
   volume = {14},
   year = {2021}
}
@article{Hhne2017,
   abstract = {Measuring attitudes and opinions employing agree/disagree (A/D) questions is a common method in social research because it appears to be possible to measure different constructs with identical response scales. However, theoretical considerations suggest that A/D questions require a considerable cognitive processing. Item-specific (IS) questions, in contrast, offer content-related response categories, implying less cognitive processing. To investigate the respective cognitive effort and response quality associated with A/D and IS questions, we conducted a web-based experiment with 1,005 students. Cognitive effort was assessed by response times and answer changes. Response quality, in contrast, was assessed by different indicators such as dropouts. According to our results, single IS questions require higher cognitive effort than single A/D questions in terms of response times. Moreover, our findings show substantial differences in processing single and grid questions.},
   author = {Jan Karem Höhne and Stephan Schlosser and Dagmar Krebs},
   doi = {10.1177/1525822X17710640},
   issn = {15523969},
   issue = {4},
   journal = {Field Methods},
   month = {11},
   pages = {365-382},
   publisher = {SAGE Publications Inc.},
   title = {Investigating Cognitive Effort and Response Quality of Question Formats in Web Surveys Using Paradata},
   volume = {29},
   year = {2017}
}
@article{Huang2012,
   abstract = {Purpose: Responses provided by unmotivated survey participants in a careless, haphazard, or random fashion can threaten the quality of data in psychological and organizational research. The purpose of this study was to summarize existing approaches to detect insufficient effort responding (IER) to low-stakes surveys and to comprehensively evaluate these approaches. Design/Methodology/Approach: In an experiment (Study 1) and a nonexperimental survey (Study 2), 725 undergraduates responded to a personality survey online. Findings: Study 1 examined the presentation of warnings to respondents as a means of deterrence and showed the relative effectiveness of four indices for detecting IE responses: response time, long string, psychometric antonyms, and individual reliability coefficients. Study 2 demonstrated that the detection indices measured the same underlying construct and showed the improvement of psychometric properties (item interrelatedness, facet dimensionality, and factor structure) after removing IE respondents identified by each index. Three approaches (response time, psychometric antonyms, and individual reliability) with high specificity and moderate sensitivity were recommended as candidates for future application in survey research. Implications: The identification of effective IER indices may help researchers ensure the quality of their low-stake survey data. Originality/value: This study is a first attempt to comprehensively evaluate IER detection methods using both experimental and nonexperimental designs. Results from both studies corroborated each other in suggesting the three more effective approaches. This study also provided convergent validity evidence regarding various indices for IER. © 2011 Springer Science+Business Media, LLC.},
   author = {Jason L. Huang and Paul G. Curran and Jessica Keeney and Elizabeth M. Poposki and Richard P. DeShon},
   doi = {10.1007/s10869-011-9231-8},
   issn = {08893268},
   issue = {1},
   journal = {Journal of Business and Psychology},
   keywords = {Careless responding,Data screening,Inconsistent responding,Online surveys,Random responding},
   month = {3},
   pages = {99-114},
   title = {Detecting and deterring insufficient effort responding to surveys},
   volume = {27},
   year = {2012}
}
@article{Kowalski2018,
   abstract = {Extant empirical research, despite some theoretical descriptions, has consistently demonstrated that the Dark Triad is not related to general mental ability. In the present study, we investigated the relationship between the Dark Triad of personality (narcissism, Machiavellianism, and psychopathy) and fluid intelligence. A sample of 128 Polish high school students (Mage = 16.89 years; SDage = 0.31; 28.1% of the sample were boys) completed the Polish translation of the Short Dark Triad and the Raven's Standard Progressive Matrices. Hypotheses were tested using a structural equation model, which fit the data well. As predicted, we found that of the three Dark Triad traits, only Machiavellianism was significantly predicted by fluid intelligence. Our findings are discussed in light of previous research and theory.},
   author = {Christopher Marcin Kowalski and Katarzyna Kwiatkowska and Maria Magdalena Kwiatkowska and Klaudia Ponikiewska and Radosław Rogoza and Julie Aitken Schermer},
   doi = {10.1016/j.paid.2018.06.049},
   issn = {01918869},
   journal = {Personality and Individual Differences},
   keywords = {Dark Triad,Intelligence,Machiavellianism,Narcissism,Psychopathy},
   month = {12},
   pages = {1-6},
   publisher = {Elsevier Ltd},
   title = {The Dark Triad traits and intelligence: Machiavellians are bright, and narcissists and psychopaths are ordinary},
   volume = {135},
   year = {2018}
}
@article{Krosnick1991,
   abstract = {This paper proposes that when optimally answering a survey question would require substantial cognitive effort, some repondents simply provide a satisfactory answer instead. This behaviour, called satisficing, can take the form of either (1) incomplete or biased information retrieval and/or information integration, or (2) no information retrieval or integration at all. Satisficing may lead respondents to employ a variety of response strategies, including choosing the first response alternative that seems to constitute a reasonable answer, agreeing with an assertion made by a question, endorsing the status quo instead of endorsing social change, failing to differentiate among a set of diverse objects in ratings, saying ‘don't know’ instead of reporting an opinion, and randomly choosing among the response alternatives offered. This paper specifies a wide range of factors that are likely to encourage satisficing, and reviews relevant evidence evaluating these speculations. Many useful directions for future research are suggested. Copyright © 1991 John Wiley & Sons, Ltd},
   author = {Jon A. Krosnick},
   doi = {10.1002/acp.2350050305},
   issn = {10990720},
   issue = {3},
   journal = {Applied Cognitive Psychology},
   pages = {213-236},
   title = {Response strategies for coping with the cognitive demands of attitude measures in surveys},
   volume = {5},
   year = {1991}
}
@article{Kuznetsova2017,
   author = {Alexandra Kuznetsova and Per B. Brockhoff and Rune H. B. Christensen},
   doi = {10.18637/jss.v082.i13},
   issn = {1548-7660},
   issue = {13},
   journal = {Journal of Statistical Software},
   pages = {1-26},
   title = {lmerTest Package: Tests in Linear Mixed Effects Models},
   volume = {82},
   year = {2017}
}
@article{Lenzner2012,
   abstract = {Many studies have shown that vague or ambiguous questions are often interpreted idiosyncratically by respondents and thus can increase measurement error. This article provides some evidence that the cognitive effort required to comprehend survey questions affects data quality in a similar way. A web survey experiment revealed that respondents receiving less comprehensible questions provided lower-quality responses (as indicated by breakoff rates, number of nonsubstantive responses, number of neutral responses, and over-time consistency) than respondents receiving control questions that were easier to comprehend. Moreover, interaction effects of question comprehensibility with respondents' verbal skills and their motivation to answer surveys were found. These findings indicate that survey designers should minimize the cognitive effort required to comprehend their questions and the article suggests specific ways how to do so. © The Author(s) 2012.},
   author = {Timo Lenzner},
   doi = {10.1177/1525822X12448166},
   issn = {1525822X},
   issue = {4},
   journal = {Field Methods},
   keywords = {question wording,questionnaire design,response effects,response quality,web survey},
   month = {11},
   pages = {409-428},
   title = {Effects of Survey Question Comprehensibility on Response Quality},
   volume = {24},
   year = {2012}
}
@article{Lenzner2010,
   abstract = {An important objective in survey question design is to write clear questions that respondents find easy to understand and to answer. This contribution identifies the factors that influence question clarity. Theoretical and empirical evidence from psycholinguistics suggests that specific text features (e.g., low-frequency words (LFRWs), left-embedded syntax) cause comprehension difficulties and impose a high cognitive burden on respondents. To examine the effect of seven different text features on question clarity, an online experiment was conducted in which well-formulated questions were compared to suboptimal counterparts. The cognitive burden of the questions was assessed with response times. Data quality was compared in terms of drop-out rates and survey satisficing behaviour. The results show that at least six of the text features are relevant for the clarity of a question. We provide a detailed explanation of these text features and advise survey designers to avoid them when crafting questions. Copyright © 2009 John Wiley & Sons, Ltd. Copyright © 2009 John Wiley & Sons, Ltd.},
   author = {Timo Lenzner and Lars Kaczmirek and Alwine Lenzner},
   doi = {10.1002/acp.1602},
   issn = {08884080},
   issue = {7},
   journal = {Applied Cognitive Psychology},
   month = {10},
   pages = {1003-1020},
   title = {Cognitive burden of survey questions and response times: A psycholinguistic experiment},
   volume = {24},
   year = {2010}
}
@phdthesis{Nguyen2017,
   author = {Hung Loan T. Nguyen},
   institution = {Middle Tennessee State University},
   title = {Tired of survey fatigue? Insufficient effort responding due to survey fatigue},
   year = {2017}
}
@misc{RCoreTeam2019,
   author = {R Core Team},
   city = {Vienna, Austria},
   publisher = {R Foundation for Statistical Computing},
   title = {R: A language and environment for statistical computing},
   url = {http://www.R-project.org/},
   year = {2019}
}
@article{Reimers2023,
   abstract = {Person-fit analyses are commonly used to detect aberrant responding in self-report data. Nonparametric person fit statistics do not require fitting a parametric test theory model and have performed well compared to other person-fit statistics. However, detection of aberrant responding has primarily focused on dominance response data, thus the effectiveness of person-fit statistics in detecting different aberrant behaviors in ideal point data is unclear. This study compares the performance of nonparametric person-fit statistics in unfolding and dominance model contexts. Results for dominance data indicate that increases in detection rates depend, among other factors, on type of aberrant responding and person-fit statistic used. The detection of aberrant responses in ideal point data was ineffective using four nonparametric person-fit statistics, with slightly higher type I error and power less than 0.25. Additional research is needed to identify or develop nonparametric or parametric person-fit statistics effective for aberrant behavior exhibited in ideal point data.},
   author = {Jennifer Reimers and Ronna C. Turner and Jorge N. Tendeiro and Wen Juo Lo and Elizabeth Keiffer},
   doi = {10.1080/15366367.2023.2165891},
   issn = {15366359},
   issue = {4},
   journal = {Measurement},
   keywords = {Nonparametric,aberrant,dominance,ideal-point,person-fit statistics,response models},
   note = {Purpose: examine performance of several person-fit statistics in identifying aberrant response tendencies when applied to data that fit one of two underlying response processes (dominance & ideal).<br/>},
   pages = {232-253},
   publisher = {Routledge},
   title = {Performance of Nonparametric Person-Fit Statistics with Unfolding versus Dominance Response Models},
   volume = {21},
   year = {2023}
}
@misc{Revelle2024,
   author = {William Revelle},
   title = {psych: Procedures for Psychological, Psychometric, and Personality Research},
   url = {https://CRAN.R-project.org/package=psych},
   year = {2024}
}
@article{Tanco2023,
   abstract = {The stereotype that children who are more able solve tasks quicker than their less capable peers exists both in and outside education. The F > C phenomenon and the distance–difficulty hypothesis offer alternative explanations of the time needed to complete a task; the former by the response correctness and the latter by the relative difference between the difficulty of the task and the ability of the examinee. To test these alternative explanations, we extracted IRT-based ability estimates and task difficulties from a sample of 514 children, 53% girls, M(age) = 10.3 years; who answered 29 Piagetian balance beam tasks. We used the answer correctness and task difficulty as predictors in multilevel regression models when controlling for children’s ability levels. Our results challenge the ‘faster equals smarter’ stereotype. We show that ability levels predict the time needed to solve a task when the task is solved incorrectly, though only with moderately and highly difficult items. Moreover, children with higher ability levels take longer to answer items incorrectly, and tasks equal to children’s ability levels take more time than very easy or difficult tasks. We conclude that the relationship between ability, task difficulty, and answer correctness is complex, and warn education professionals against basing their professional judgment on students’ quickness.},
   author = {Martin Tancoš and Edita Chvojka and Michal Jabůrek and Šárka Portešová},
   doi = {10.3390/jintelligence11040063},
   issn = {20793200},
   issue = {4},
   journal = {Journal of Intelligence},
   keywords = {F > C phenomenon,IRT,Thissen’s model,balance beam task,distance–difficulty hypothesis,fluid intelligence,game-based assessment,response time},
   month = {4},
   publisher = {MDPI},
   title = {Faster ≠ Smarter: Children with Higher Levels of Ability Take Longer to Give Incorrect Answers, Especially When the Task Matches Their Ability},
   volume = {11},
   year = {2023}
}
@inbook{Thissen1983,
   author = {David Thissen},
   booktitle = {New horizons in testing: Latent trait test theory and computerized adaptive testing},
   pages = {179-203},
   title = {Timed testing- An approach using Item Response Theory},
   year = {1983}
}
@article{Ulitzsch2022,
   abstract = {Careless and insufficient effort responding (C/IER) can pose a major threat to data quality and, as such, to validity of inferences drawn from questionnaire data. A rich body of methods aiming at its detection has been developed. Most of these methods can detect only specific types of C/IER patterns. However, typically different types of C/IER patterns occur within one data set and need to be accounted for. We present a model-based approach for detecting manifold manifestations of C/IER at once. This is achieved by leveraging response time (RT) information available from computer-administered questionnaires and integrating theoretical considerations on C/IER with recent psychometric modeling approaches. The approach a) takes the specifics of attentive response behavior on questionnaires into account by incorporating the distance–difficulty hypothesis, b) allows for attentiveness to vary on the screen-by-respondent level, c) allows for respondents with different trait and speed levels to differ in their attentiveness, and d) at once deals with various response patterns arising from C/IER. The approach makes use of item-level RTs. An adapted version for aggregated RTs is presented that supports screening for C/IER behavior on the respondent level. Parameter recovery is investigated in a simulation study. The approach is illustrated in an empirical example, comparing different RT measures and contrasting the proposed model-based procedure against indicator-based multiple-hurdle approaches.},
   author = {Esther Ulitzsch and Steffi Pohl and Lale Khorramdel and Ulf Kroehne and Matthias von Davier},
   doi = {10.1007/s11336-021-09817-7},
   issn = {18600980},
   issue = {2},
   journal = {Psychometrika},
   keywords = {careless responses,data screening,item response theory,mixture modeling,response times},
   month = {6},
   pages = {593-619},
   pmid = {34855118},
   publisher = {Springer},
   title = {A Response-Time-Based Latent Response Mixture Model for Identifying and Modeling Careless and Insufficient Effort Responding in Survey Data},
   volume = {87},
   year = {2022}
}
@techReport{VanDerLinden2003,
   abstract = {A lognormal model for response times is used to check response times for aberrances in exami-nee behavior on computerized adaptive tests. Both classical procedures and Bayesian posterior predictive checks are presented. For a fixed examinee, responses and response times are independent; checks based on response times offer thus information independent of the results of checks on response patterns. Empirical examples of the use of classical and Bayesian checks for detecting two different types of aberrances in response times are presented. The detection rates for the Bayesian checks outperformed those for the classical checks, but at the cost of higher false-alarm rates. A guideline for the choice between the two types of checks is offered.},
   author = {Van Der Linden, Wim J and Van Krimpen-Stoop, Edith M L A },
   issue = {2},
   journal = {PSYCHOMETRIKA},
   keywords = {aberrant response patterns,computerized adaptive testing,person misfit,posterior predictive checks,residual analysis,response times},
   pages = {251-265},
   title = {Using response times to detect aberrant responses in computerized adaptive testing},
   volume = {68},
   year = {2003}
}
@misc{Yentes2023,
   author = {Richard Yentes and Francisco Wilhelm},
   title = {careless: Procedures for computing indices of careless responding},
   year = {2023}
}